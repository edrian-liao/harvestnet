{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the ResNet-50 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a basic ResNet-50 Model and check to see if results match with paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunsakai/opt/anaconda3/envs/harvest/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torchmetrics import Accuracy, F1Score, AUROC, Precision, Recall\n",
    "from accelerate import Accelerator\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from dataset import SkysatLabelled\n",
    "from tools.config import Config_Resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunsakai/opt/anaconda3/envs/harvest/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/evaluation parameters:\n",
      "{'working_dir': '/atlas2/u/XXXX-2/harvest-piles', 'dataset_path': '/atlas2/u/XXXX-2/datasets', 'wandb_project': 'harvest-piles', 'wandb_group': 'resnet50', 'seed': 2023, 'scheduler': 'one_cycle_lr', 'lr': 0.001, 'optimizer': 'madgrad', 'mixed_precision': 'fp16', 'num_train_epochs': 30, 'batch_size': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshunsakai1217\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /atlas2/u/XXXX-2/harvest-piles/wandb/ wasn't writable, using system temp directory.\n",
      "wandb: WARNING Path /atlas2/u/XXXX-2/harvest-piles/wandb/ wasn't writable, using system temp directory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/var/folders/5y/klq_vsbn365gv4d3wvljm_lr0000gn/T/wandb/run-20240416_015242-d48wnre4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shunsakai1217/harvest-piles/runs/d48wnre4/workspace' target=\"_blank\">absurd-energy-3</a></strong> to <a href='https://wandb.ai/shunsakai1217/harvest-piles' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shunsakai1217/harvest-piles' target=\"_blank\">https://wandb.ai/shunsakai1217/harvest-piles</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shunsakai1217/harvest-piles/runs/d48wnre4/workspace' target=\"_blank\">https://wandb.ai/shunsakai1217/harvest-piles/runs/d48wnre4/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:d48wnre4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-energy-3</strong> at: <a href='https://wandb.ai/shunsakai1217/harvest-piles/runs/d48wnre4/workspace' target=\"_blank\">https://wandb.ai/shunsakai1217/harvest-piles/runs/d48wnre4/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/var/folders/5y/klq_vsbn365gv4d3wvljm_lr0000gn/T/wandb/run-20240416_015242-d48wnre4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:d48wnre4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/var/folders/5y/klq_vsbn365gv4d3wvljm_lr0000gn/T/wandb/run-20240416_015251-n67z810w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shunsakai1217/harvest-piles/runs/n67z810w/workspace' target=\"_blank\">desert-aardvark-4</a></strong> to <a href='https://wandb.ai/shunsakai1217/harvest-piles' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shunsakai1217/harvest-piles' target=\"_blank\">https://wandb.ai/shunsakai1217/harvest-piles</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shunsakai1217/harvest-piles/runs/n67z810w/workspace' target=\"_blank\">https://wandb.ai/shunsakai1217/harvest-piles/runs/n67z810w/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# CONFIG\n",
    "# -----------------\n",
    "\n",
    "config = Config_Resnet()\n",
    "\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    log_with=\"wandb\",\n",
    "    mixed_precision=config.mixed_precision,\n",
    ")\n",
    "device = accelerator.device\n",
    "\n",
    "# Log on each process the small summary:\n",
    "if accelerator.is_main_process:\n",
    "    print(f\"Training/evaluation parameters:\")\n",
    "    print(config.__dict__)\n",
    "\n",
    "accelerator.init_trackers(\n",
    "    config.wandb_project,\n",
    "    config=config,\n",
    "    init_kwargs={\n",
    "        \"wandb\": {\n",
    "            \"group\": config.wandb_group,\n",
    "            \"reinit\": True,\n",
    "            \"dir\": os.path.join(config.working_dir),\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "LOGGING = True\n",
    "if LOGGING:\n",
    "    accelerator.init_trackers(\n",
    "        config.wandb_project,\n",
    "        config=config,\n",
    "        init_kwargs={\n",
    "            \"wandb\": {\n",
    "                \"group\": config.wandb_group,\n",
    "                \"reinit\": True,\n",
    "                \"dir\": os.path.join(config.working_dir),\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# DATASET\n",
    "# -----------------\n",
    "if accelerator.is_main_process:\n",
    "    print(\"Loading datasets\")\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        # torchvision.transforms.RandomHorizontalFlip(),\n",
    "        # torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.412, 0.368, 0.326], std=[0.110, 0.097, 0.098]\n",
    "        ),  # our dataset vals\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@381.929] global loadsave.cpp:248 findDecoder imread_('/atlas2/u/XXXX-2/datasets/merged/746.tif'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /Users/runner/miniforge3/conda-bld/libopencv_1706393511267/work/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SkysatLabelled(\n\u001b[1;32m      4\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mdataset_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m     transform,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# take a look at train_dataset\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     12\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SkysatLabelled(\n\u001b[1;32m     13\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mdataset_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     14\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mdataset_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m     transform,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m SkysatLabelled(\n\u001b[1;32m     19\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mdataset_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     20\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mdataset_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m     transform,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/CS661/Final Project/harvestnet/notebooks/../dataset.py:23\u001b[0m, in \u001b[0;36mSkysatLabelled.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[index])\n\u001b[1;32m     22\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(path)\n\u001b[0;32m---> 23\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[index])\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /Users/runner/miniforge3/conda-bld/libopencv_1706393511267/work/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATASETS\n",
    "ROOT_PATH = \"../Dataset\"\n",
    "train_dataset = SkysatLabelled(\n",
    "    os.path.join(ROOT_PATH, \"train.csv\"),\n",
    "    os.path.join(config.dataset_path, \"merged\"),\n",
    "    transform,\n",
    ")\n",
    "\n",
    "train_dataset = SkysatLabelled(\n",
    "    os.path.join(config.dataset_path, \"train.csv\"),\n",
    "    os.path.join(config.dataset_path, \"merged\"),\n",
    "    transform,\n",
    ")\n",
    "\n",
    "test_dataset = SkysatLabelled(\n",
    "    os.path.join(config.dataset_path, \"test.csv\"),\n",
    "    os.path.join(config.dataset_path, \"merged\"),\n",
    "    transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunsakai/opt/anaconda3/envs/harvest/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/evaluation parameters:\n",
      "{'working_dir': '/atlas2/u/XXXX-2/harvest-piles', 'dataset_path': '/atlas2/u/XXXX-2/datasets', 'wandb_project': 'harvest-piles', 'wandb_group': 'resnet50', 'seed': 2023, 'scheduler': 'one_cycle_lr', 'lr': 0.001, 'optimizer': 'madgrad', 'mixed_precision': 'fp16', 'num_train_epochs': 30, 'batch_size': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# MODEL\n",
    "# -----------------\n",
    "if accelerator.is_main_process:\n",
    "    print(\"Loading model\")\n",
    "\n",
    "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
    "\n",
    "\n",
    "class ResNet50(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.resnet50 = torchvision.models.resnet50(weights=model_weights)\n",
    "        num_features = self.resnet50.fc.out_features\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_features, num_classes),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.resnet50(x)\n",
    "        return self.fc(logits)\n",
    "\n",
    "\n",
    "out_classes = 1\n",
    "model = ResNet50(out_classes)\n",
    "\n",
    "# -----------------\n",
    "# OPTIMIZER, SCHEDULER\n",
    "# -----------------\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "accuracy = Accuracy(task=\"binary\")\n",
    "f1_score = F1Score(task=\"binary\")\n",
    "auroc = AUROC(task=\"binary\")\n",
    "precision = Precision(task=\"binary\")\n",
    "recall = Recall(task=\"binary\")\n",
    "\n",
    "FOUND_LR = config.lr  # For OneCycleLR\n",
    "# FOUND_LR = 0.001 # For ExponentialLR\n",
    "\n",
    "\n",
    "params = [\n",
    "    {\"params\": model.resnet50.conv1.parameters(), \"lr\": FOUND_LR / 10},\n",
    "    {\"params\": model.resnet50.bn1.parameters(), \"lr\": FOUND_LR / 10},\n",
    "    {\"params\": model.resnet50.layer1.parameters(), \"lr\": FOUND_LR / 8},\n",
    "    {\"params\": model.resnet50.layer2.parameters(), \"lr\": FOUND_LR / 6},\n",
    "    {\"params\": model.resnet50.layer3.parameters(), \"lr\": FOUND_LR / 4},\n",
    "    {\"params\": model.resnet50.layer4.parameters(), \"lr\": FOUND_LR / 2},\n",
    "    {\"params\": model.resnet50.fc.parameters()},\n",
    "]\n",
    "if config.optimizer == \"adam\":\n",
    "    optimizer = torch.optim.Adam(params, lr=config.lr)\n",
    "elif config.optimizer == \"madgrad\":\n",
    "    from optim.madgrad import MADGRAD\n",
    "\n",
    "    optimizer = MADGRAD(params, lr=config.lr)\n",
    "\n",
    "STEPS_PER_EPOCH = len(train_dataloader)\n",
    "TOTAL_STEPS = config.num_train_epochs * STEPS_PER_EPOCH\n",
    "MAX_LRS = [p[\"lr\"] for p in optimizer.param_groups]\n",
    "\n",
    "if config.scheduler == \"one_cycle_lr\":\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=MAX_LRS, total_steps=TOTAL_STEPS\n",
    "    )\n",
    "\n",
    "# -----------------\n",
    "# ACCELERATOR\n",
    "# -----------------\n",
    "model, optimizer, train_dataloader, test_dataloader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, test_dataloader, scheduler\n",
    ")\n",
    "accuracy, f1_score, auroc, precision, recall = accelerator.prepare(\n",
    "    accuracy, f1_score, auroc, precision, recall\n",
    ")\n",
    "\n",
    "# -----------------\n",
    "# TRAIN\n",
    "# -----------------\n",
    "if accelerator.is_main_process:\n",
    "    print(\"Begin train\")\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, scheduler):\n",
    "    epoch_loss = 0\n",
    "    sum_metrics = defaultdict(float)\n",
    "\n",
    "    model.train()\n",
    "    for x, y, _ in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        evals = {\n",
    "            \"acc\": accuracy(pred, y),\n",
    "            \"f1\": f1_score(pred, y),\n",
    "            \"auroc\": auroc(pred, y),\n",
    "            \"precision\": precision(pred, y),\n",
    "            \"recall\": recall(pred, y),\n",
    "        }\n",
    "        for k, v in evals.items():\n",
    "            sum_metrics[k] += v\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        if config.scheduler == \"one_cycle_lr\":\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if config.scheduler != \"one_cycle_lr\":\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    avg_metrics = {k: v / len(iterator) for k, v in sum_metrics.items()}\n",
    "\n",
    "    return epoch_loss, avg_metrics\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    sum_metrics = defaultdict(float)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in iterator:\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            evals = {\n",
    "                \"acc\": accuracy(pred, y),\n",
    "                \"f1\": f1_score(pred, y),\n",
    "                \"auroc\": auroc(pred, y),\n",
    "                \"precision\": precision(pred, y),\n",
    "                \"recall\": recall(pred, y),\n",
    "            }\n",
    "            for k, v in evals.items():\n",
    "                sum_metrics[k] += v\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    avg_metrics = {k: v / len(iterator) for k, v in sum_metrics.items()}\n",
    "\n",
    "    return epoch_loss, avg_metrics\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "best_val = 0\n",
    "\n",
    "if LOGGING:\n",
    "    print(\"Batch size:\", config.batch_size)\n",
    "    print(\"Batches per epoch:\", len(train_dataloader))\n",
    "for epoch in range(config.num_train_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_metrics = train(\n",
    "        model, train_dataloader, optimizer, criterion, scheduler\n",
    "    )\n",
    "    valid_loss, val_metrics = evaluate(model, test_dataloader, criterion)\n",
    "    if LOGGING:\n",
    "        wandb.log({\"train.loss\": train_loss}, commit=False)\n",
    "        wandb.log({\"train\": train_metrics}, commit=False)\n",
    "        wandb.log({\"val.loss\": valid_loss}, commit=False)\n",
    "        wandb.log({\"val\": val_metrics}, commit=False)\n",
    "\n",
    "    if val_metrics[\"acc\"] > best_val:\n",
    "        best_val = val_metrics[\"acc\"]\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save(\n",
    "            model.state_dict(), \"/atlas2/u/XXXX-2/harvest-piles/results/resnet.pt\"\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(\n",
    "            f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_metrics['acc'] * 100:6.2f}%\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tValid Loss: {valid_loss:.3f} | Valid Acc: {val_metrics['acc'] * 100:6.2f}%\"\n",
    "        )\n",
    "\n",
    "    if LOGGING:\n",
    "        wandb.log({\"epoch\": epoch}, commit=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs661",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
