{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the ResNet-50 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a basic ResNet-50 Model and check to see if results match with paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torchmetrics import Accuracy, F1Score, AUROC, Precision, Recall\n",
    "from accelerate import Accelerator\n",
    "from collections import defaultdict\n",
    "\n",
    "from dataset import SkysatLabelled\n",
    "from tools.config import Config_Resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# CONFIG\n",
    "# -----------------\n",
    "\n",
    "config = Config_Resnet()\n",
    "\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    log_with=\"wandb\",\n",
    "    mixed_precision=config.mixed_precision,\n",
    ")\n",
    "device = accelerator.device\n",
    "\n",
    "# Log on each process the small summary:\n",
    "if accelerator.is_main_process:\n",
    "    print(f\"Training/evaluation parameters:\")\n",
    "    print(config.__dict__)\n",
    "\n",
    "accelerator.init_trackers(\n",
    "    config.wandb_project,\n",
    "    config=config,\n",
    "    init_kwargs={\n",
    "        \"wandb\": {\n",
    "            \"group\": config.wandb_group,\n",
    "            \"reinit\": True,\n",
    "            \"dir\": os.path.join(config.working_dir),\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "LOGGING = True\n",
    "if LOGGING:\n",
    "    accelerator.init_trackers(\n",
    "        config.wandb_project,\n",
    "        config=config,\n",
    "        init_kwargs={\n",
    "            \"wandb\": {\n",
    "                \"group\": config.wandb_group,\n",
    "                \"reinit\": True,\n",
    "                \"dir\": os.path.join(config.working_dir),\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# DATASET\n",
    "# -----------------\n",
    "if accelerator.is_main_process:\n",
    "    print(\"Loading datasets\")\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        # torchvision.transforms.RandomHorizontalFlip(),\n",
    "        # torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.412, 0.368, 0.326], std=[0.110, 0.097, 0.098]\n",
    "        ),  # our dataset vals\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = SkysatLabelled(\n",
    "    os.path.join(config.dataset_path, \"train.csv\"),\n",
    "    os.path.join(config.dataset_path, \"merged\"),\n",
    "    transform,\n",
    ")\n",
    "test_dataset = SkysatLabelled(\n",
    "    os.path.join(config.dataset_path, \"test.csv\"),\n",
    "    os.path.join(config.dataset_path, \"merged\"),\n",
    "    transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# -----------------\n",
    "# MODEL\n",
    "# -----------------\n",
    "if accelerator.is_main_process:\n",
    "    print(\"Loading model\")\n",
    "\n",
    "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
    "\n",
    "\n",
    "class ResNet50(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.resnet50 = torchvision.models.resnet50(weights=model_weights)\n",
    "        num_features = self.resnet50.fc.out_features\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_features, num_classes),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.resnet50(x)\n",
    "        return self.fc(logits)\n",
    "\n",
    "\n",
    "out_classes = 1\n",
    "model = ResNet50(out_classes)\n",
    "\n",
    "# -----------------\n",
    "# OPTIMIZER, SCHEDULER\n",
    "# -----------------\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "accuracy = Accuracy(task=\"binary\")\n",
    "f1_score = F1Score(task=\"binary\")\n",
    "auroc = AUROC(task=\"binary\")\n",
    "precision = Precision(task=\"binary\")\n",
    "recall = Recall(task=\"binary\")\n",
    "\n",
    "FOUND_LR = config.lr  # For OneCycleLR\n",
    "# FOUND_LR = 0.001 # For ExponentialLR\n",
    "\n",
    "\n",
    "params = [\n",
    "    {\"params\": model.resnet50.conv1.parameters(), \"lr\": FOUND_LR / 10},\n",
    "    {\"params\": model.resnet50.bn1.parameters(), \"lr\": FOUND_LR / 10},\n",
    "    {\"params\": model.resnet50.layer1.parameters(), \"lr\": FOUND_LR / 8},\n",
    "    {\"params\": model.resnet50.layer2.parameters(), \"lr\": FOUND_LR / 6},\n",
    "    {\"params\": model.resnet50.layer3.parameters(), \"lr\": FOUND_LR / 4},\n",
    "    {\"params\": model.resnet50.layer4.parameters(), \"lr\": FOUND_LR / 2},\n",
    "    {\"params\": model.resnet50.fc.parameters()},\n",
    "]\n",
    "if config.optimizer == \"adam\":\n",
    "    optimizer = torch.optim.Adam(params, lr=config.lr)\n",
    "elif config.optimizer == \"madgrad\":\n",
    "    from optim.madgrad import MADGRAD\n",
    "\n",
    "    optimizer = MADGRAD(params, lr=config.lr)\n",
    "\n",
    "STEPS_PER_EPOCH = len(train_dataloader)\n",
    "TOTAL_STEPS = config.num_train_epochs * STEPS_PER_EPOCH\n",
    "MAX_LRS = [p[\"lr\"] for p in optimizer.param_groups]\n",
    "\n",
    "if config.scheduler == \"one_cycle_lr\":\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=MAX_LRS, total_steps=TOTAL_STEPS\n",
    "    )\n",
    "\n",
    "# -----------------\n",
    "# ACCELERATOR\n",
    "# -----------------\n",
    "model, optimizer, train_dataloader, test_dataloader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, test_dataloader, scheduler\n",
    ")\n",
    "accuracy, f1_score, auroc, precision, recall = accelerator.prepare(\n",
    "    accuracy, f1_score, auroc, precision, recall\n",
    ")\n",
    "\n",
    "# -----------------\n",
    "# TRAIN\n",
    "# -----------------\n",
    "if accelerator.is_main_process:\n",
    "    print(\"Begin train\")\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, scheduler):\n",
    "    epoch_loss = 0\n",
    "    sum_metrics = defaultdict(float)\n",
    "\n",
    "    model.train()\n",
    "    for x, y, _ in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        evals = {\n",
    "            \"acc\": accuracy(pred, y),\n",
    "            \"f1\": f1_score(pred, y),\n",
    "            \"auroc\": auroc(pred, y),\n",
    "            \"precision\": precision(pred, y),\n",
    "            \"recall\": recall(pred, y),\n",
    "        }\n",
    "        for k, v in evals.items():\n",
    "            sum_metrics[k] += v\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        if config.scheduler == \"one_cycle_lr\":\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if config.scheduler != \"one_cycle_lr\":\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    avg_metrics = {k: v / len(iterator) for k, v in sum_metrics.items()}\n",
    "\n",
    "    return epoch_loss, avg_metrics\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    sum_metrics = defaultdict(float)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in iterator:\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            evals = {\n",
    "                \"acc\": accuracy(pred, y),\n",
    "                \"f1\": f1_score(pred, y),\n",
    "                \"auroc\": auroc(pred, y),\n",
    "                \"precision\": precision(pred, y),\n",
    "                \"recall\": recall(pred, y),\n",
    "            }\n",
    "            for k, v in evals.items():\n",
    "                sum_metrics[k] += v\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    avg_metrics = {k: v / len(iterator) for k, v in sum_metrics.items()}\n",
    "\n",
    "    return epoch_loss, avg_metrics\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "best_val = 0\n",
    "\n",
    "if LOGGING:\n",
    "    print(\"Batch size:\", config.batch_size)\n",
    "    print(\"Batches per epoch:\", len(train_dataloader))\n",
    "for epoch in range(config.num_train_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_metrics = train(\n",
    "        model, train_dataloader, optimizer, criterion, scheduler\n",
    "    )\n",
    "    valid_loss, val_metrics = evaluate(model, test_dataloader, criterion)\n",
    "    if LOGGING:\n",
    "        wandb.log({\"train.loss\": train_loss}, commit=False)\n",
    "        wandb.log({\"train\": train_metrics}, commit=False)\n",
    "        wandb.log({\"val.loss\": valid_loss}, commit=False)\n",
    "        wandb.log({\"val\": val_metrics}, commit=False)\n",
    "\n",
    "    if val_metrics[\"acc\"] > best_val:\n",
    "        best_val = val_metrics[\"acc\"]\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save(\n",
    "            model.state_dict(), \"/atlas2/u/XXXX-2/harvest-piles/results/resnet.pt\"\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(\n",
    "            f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_metrics['acc'] * 100:6.2f}%\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tValid Loss: {valid_loss:.3f} | Valid Acc: {val_metrics['acc'] * 100:6.2f}%\"\n",
    "        )\n",
    "\n",
    "    if LOGGING:\n",
    "        wandb.log({\"epoch\": epoch}, commit=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs661",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
